
/*
 Blog backend Test data
 --------------------------
 data.sql
 ----------------------------------------------------------------------------
 MariaDB/MySQL Insert Data commands for Jdbc default 
 Requires MariaDB version 10.2.1 or MySQL version 8.0.13 and aftrward. 
 (C)opyright Panos Zafeiropoulos - 2024


 ----------------------------------------------------------------------------
 Last update: 241122 - PZ
 -----------------------------------------------------------------------------
*/

BEGIN;
INSERT INTO `users` (`userName`, `userPassword`, `userEmail`, `userIsEnabled`, `userSlugName`) VALUES ('Panagiotis', 'panos_passw', 'admin@genmail.com', 1, 'panos');
INSERT INTO `users` (`userName`, `userPassword`, `userEmail`, `userIsEnabled`, `userSlugName`) VALUES ('Kleoniki', 'niki_passw', 'niki@genmail.com', 1, 'niki');
INSERT INTO `users` (`userName`, `userPassword`, `userEmail`, `userIsEnabled`, `userSlugName`) VALUES ('Ioannis', 'john_passw', 'john@genmail.com',  1, 'john');
INSERT INTO `users` (`userName`, `userPassword`, `userEmail`, `userIsEnabled`, `userSlugName`) VALUES ('`Styliani', 'stela_passw', 'stela@genmail.com',  1, 'stela');
INSERT INTO `users` (`userName`, `userPassword`, `userEmail`, `userIsEnabled`, `userIsAuthor`, `userSlugName`) VALUES ('Paraskevi', 'voula_passw', 'voula@genmail.com',  1, 0, 'voula');
COMMIT;


BEGIN;
INSERT INTO testcategories(categoryTitle) VALUES ('AI-DS');
INSERT INTO testcategories(categoryTitle) VALUES ('APIs');
INSERT INTO testcategories(categoryTitle) VALUES ('SOLID');
INSERT INTO testcategories(categoryTitle) VALUES ('SCRUM');
INSERT INTO testcategories(categoryTitle) VALUES ('Microservices');
COMMIT;

BEGIN;
INSERT INTO testarticles(categoryId,articleTitle,articleSubTitle,articleContent) VALUES (1,'Artificial intelligence','Artificial intelligence','Artificial intelligence (AI) refers to the development of computer systems that can perform tasks that typically require human intelligence. These tasks include learning, reasoning, problem-solving, perception, understanding natural language, and interacting with the environment. AI systems aim to mimic or replicate human cognitive functions to achieve specific goals and solve complex problems.');
INSERT INTO testarticles(categoryId,articleTitle,articleSubTitle,articleContent) VALUES (1,'Data Science','Data Science','Data science is a multidisciplinary field that combines various techniques and methods from statistics, mathematics, computer science, and domain-specific knowledge to extract insights and knowledge from data. It involves collecting, analyzing, interpreting, and visualizing large and complex datasets to uncover patterns, trends, and relationships that can inform decision-making and solve real-world problems.');
INSERT INTO testarticles(categoryId,articleTitle,articleSubTitle,articleContent) VALUES (1,'Machine Learning','Machine Learning','Machine learning is a subset of artificial intelligence (AI) that enables computers to learn from data and improve their performance on a task without being explicitly programmed. 
  In other words, machine learning algorithms allow systems to automatically learn and make predictions or decisions based on patterns and relationships found in data.');
INSERT INTO testarticles(categoryId,articleTitle,articleSubTitle,articleContent) VALUES (1,'EDA','Exploratory Data Analysis (EDA)','Data analysis is the whole process of examining, cleaning, transforming, and interpreting data to discover meaningful insights, patterns, and trends. 
 It involves applying various statistical and computational techniques to understand the underlying structure of the data and extract actionable information. 
         It encompasses a broad range of activities aimed at extracting insights and knowledge from data to support decision-making, problem-solving, and discovery. 
          It involves several key steps: 
          Data Collection: Gathering relevant data from various sources, such as databases, files, or APIs. 
           Data Cleaning: Identifying and correcting errors, inconsistencies, and missing values in the data.    
       Data Transformation: Preprocessing and transforming the data into a suitable format for analysis, including feature engineering and normalization.   
     Data Modeling: Applying statistical, machine learning, or computational techniques to analyze the data and derive insights.    
      Interpretation and Visualization: Interpreting the results of the analysis and communicating findings through visualizations, reports, or dashboards.    
      Data analysis can be descriptive, diagnostic, predictive, or prescriptive, depending on the goals and context of the analysis.');
INSERT INTO testarticles(categoryId,articleTitle,articleSubTitle,articleContent) VALUES (1,'Classification','Classification','We can simply define Classification as the process of recognition, understanding, and grouping of objects and ideas into preset categories or sub-populations within a large group. 
    With the assistance of these pre-categorized training datasets, classification in machine learning programs leverages a vast range of algorithms to classify future datasets into respective 
     and relevant categories for simplification.
     Classification algorithms that we use in machine learning utilize input training data for the function of predicting the similarities or probability that the data that follows will come under one 
      of the predetermined categories. One of the most familiar applications of classification is for filtering emails as spam or non-spam. 
     To put it simply, classification is a form of pattern recognition technique in Machine Learning. In the present context, classification algorithms that we apply to the train data tend to find the same pattern in future new data sets. 
     The classification process is primarily divided into two groups: Binary Classification and Multi-Class Classification.');
INSERT INTO testarticles(categoryId,articleTitle,articleSubTitle,articleContent) VALUES (1,'Clustering','Clustering/Segmentation','Clustering/Segmentation - The objective of clustering or segmentation is to group similar data points together into clusters or segments based on their inherent similarities or patterns in the data. 
      - Clustering is an unsupervised learning task, meaning that the algorithm works with unlabeled data and must discover the underlying structure or patterns in the data on its own. 
      - The goal is to partition the data into clusters such that data points within the same cluster are more similar to each other than to those in other clusters.');
INSERT INTO testarticles(categoryId,articleTitle,articleSubTitle,articleContent) VALUES (1,'Neural Networks','Neural Networks','A neural network is a machine learning program, or model, that makes decisions in a manner similar to the human brain, by using processes that mimic the way biological neurons work together to identify phenomena, weigh options and arrive at conclusions. 
  Every neural network consists of layers of nodes, or artificial neurons—an input layer, one or more hidden layers, and an output layer. 
 https://www.ibm.com/topics/neural-networks  
 In machine learning, a neural network (also artificial neural network or neural net, abbreviated ANN or NN) is a model inspired by the structure and function of biological neural networks in animal brains. 
     Deep learning algorithms inspired by the structure and function of the human brain. They consist of multiple layers of interconnected neurons and are used for various tasks, including image recognition, natural language processing, and speech recognition.');
INSERT INTO testarticles(categoryId,articleTitle,articleSubTitle,articleContent) VALUES (2,'General','General Information about APIs','API stands for Application Programming Interface. They are a set of protocols and tools that allow different software applications to communicate with each other. In simpler terms, it’s a middleman that enables different software applications to talk to each other. APIs provide a standardized way to send or receive data. 
 This concept is fundamental in software engineering because it allows applications to exchange data and functionalities. Developers can leverage services provided by other systems, even if they were built with different tools. Think of APIs as universal translators, allowing applications built in different languages to communicate seamlessly. REST, SOAP, and GraphQL are some of the most common API formats.');
INSERT INTO testarticles(categoryId,articleTitle,articleSubTitle,articleContent) VALUES (2,'Design','API Design Principles','Designing an effective API requires adherence to certain principles that help ensure consistency, usability, and maintainability. Here are some fundamental principles to consider: 
 Simplicity: APIs should be designed with simplicity in mind, making them easy to understand and use. Avoid unnecessary complexity and strive for a clean and intuitive interface. 
 Consistency: Consistency is key when it comes to API design. Maintain a consistent naming convention, structure, and behavior throughout the API to enhance developer experience and reduce confusion. 
 Separation of Concerns: APIs should have clear boundaries and responsibilities, separating concerns related to data, business logic, and presentation. 
 Backward Compatibility: When introducing changes or new versions of an API, it’s essential to maintain backward compatibility to prevent breaking existing client applications. 
 Versioning: Implement versioning strategies to manage API changes effectively and allow clients to migrate to newer versions at their own pace. 
 Security: APIs often handle sensitive data and operations, making security a critical consideration. Implement appropriate authentication, authorization, and encryption mechanisms to protect against potential threats. 
 Documentation: Well-documented APIs are essential for developers to understand how to consume and integrate them into their applications. Clear and comprehensive documentation should be provided, including examples and usage scenarios. 
 Scalability: APIs should be designed with scalability in mind, considering factors such as load balancing, caching, and horizontal scaling to handle increasing demand and traffic. 
 Testability: APIs should be designed with testability in mind, allowing for easy integration testing, load testing, and regression testing to ensure reliability and robustness.');
INSERT INTO testarticles(categoryId,articleTitle,articleSubTitle,articleContent) VALUES (2,'REST','Representational State Transfer (REST)','A REST API (Representational State Transfer Application Programming Interface) is a set of rules and conventions for building and interacting with web services. It leverages HTTP requests to facilitate communication between a client and a server, using standard HTTP methods such as GET, POST, PUT, DELETE, and PATCH. The core principles of REST include statelessness, where each request from a client to a server must contain all the information the server needs to understand and process the request, and a uniform interface, which simplifies and decouples the architecture. REST APIs are typically designed to be lightweight, making them suitable for applications that require high performance and scalability. 
 One of the key features of REST APIs is their use of resources, which are identified by URLs. A resource can represent any data object or service, such as a user, a product, or a transaction. The client interacts with these resources through a set of well-defined operations, often returning data in a standard format like JSON or XML. REST APIs are widely used in modern web development due to their simplicity, flexibility, and ability to work with a variety of data formats. They also support a broad range of data exchange patterns, from simple request-response to more complex interactions, making them a versatile choice for building distributed systems and microservices architectures.');
INSERT INTO testarticles(categoryId,articleTitle,articleSubTitle,articleContent) VALUES (2,'SOAP','Simple Object Access Protocol Application Programming Interface (SOAP)','A SOAP API (Simple Object Access Protocol Application Programming Interface) is a protocol used for exchanging structured information in the implementation of web services. It relies on XML (eXtensible Markup Language) to format messages and typically uses HTTP or SMTP as the transport protocol. Unlike REST APIs, which are based on a set of architectural principles, SOAP is a protocol that comes with a strict set of standards. It defines a set of rules for structuring messages, conducting remote procedure calls, and describing services in a machine-readable format using WSDL (Web Services Description Language). 
 
 SOAP APIs are known for their robustness and ability to handle complex transactions and secure communications. They offer built-in error handling and support for various security protocols, such as WS-Security, making them suitable for enterprise-level applications that require high reliability and security, such as banking and e-commerce platforms. The protocol''s strict standards ensure that all SOAP-based services are interoperable, regardless of the platform or programming language used. However, this rigidity can also make SOAP APIs more complex to implement and maintain compared to REST APIs. Despite this complexity, SOAP remains a popular choice for services that require a formal contract between the client and server, rigorous security features, and transactional reliability.');
INSERT INTO testarticles(categoryId,articleTitle,articleSubTitle,articleContent) VALUES (2,'GraphQL','GraphQL API','A GraphQL-based API is a flexible and efficient query language for APIs and a runtime for executing those queries. Developed by Facebook in 2012 and released publicly in 2015, GraphQL provides a more efficient, powerful, and flexible alternative to REST. It allows clients to request exactly the data they need, rather than having to receive a fixed data structure, which often includes unnecessary information. The API exposes a single endpoint, and clients can specify their exact data requirements in the query. This means the server can deliver precisely the required data, which can reduce the number of network requests and minimize data transfer. 
 
 GraphQL APIs use a schema to define the types of data and relationships between them. This schema acts as a contract between the client and the server, ensuring that all queries are valid and that the server can respond appropriately. The schema also allows for introspection, enabling clients to query the structure of the data and adjust their queries accordingly. This makes GraphQL particularly powerful for front-end developers, as they can iterate quickly without waiting for backend changes. Furthermore, GraphQL supports real-time updates through subscriptions, allowing clients to receive updated data as soon as it changes on the server, which is ideal for real-time applications like chat or live updates. Overall, GraphQL''s ability to fetch specific data, its strict type system, and real-time capabilities make it a popular choice for modern API development.');
INSERT INTO testarticles(categoryId,articleTitle,articleSubTitle,articleContent) VALUES (2,'OpenAPI-Swagger','OpenAPI-Swagger','OpenAPI and Swagger are closely related concepts in the world of API design and documentation. OpenAPI, originally known as the Swagger Specification, is a standard for defining RESTful APIs. It provides a structured way to describe the various aspects of an API, including the available endpoints, the request and response formats, the authentication methods, and more. This specification enables developers to create a detailed, machine-readable API description that can be used for a variety of purposes, such as documentation, testing, and client generation. OpenAPI has become widely adopted in the industry, providing a common language for API development and fostering better collaboration between frontend and backend teams. 
 
 Swagger, on the other hand, originated as a set of open-source tools built around the OpenAPI Specification. It includes tools like Swagger Editor, Swagger UI, and Swagger Codegen. Swagger Editor allows developers to write OpenAPI definitions with a user-friendly interface, Swagger UI provides an interactive documentation interface that lets users explore and test API endpoints, and Swagger Codegen generates client libraries, server stubs, and API documentation from an OpenAPI Specification. The relation between OpenAPI and Swagger is that OpenAPI defines the standard format for API descriptions, while Swagger provides the tools to work with these descriptions. Although the terminology has evolved, with OpenAPI becoming the official name for the specification and Swagger referring to the toolset, they are still closely intertwined and often mentioned together in the context of API development.');
INSERT INTO testarticles(categoryId,articleTitle,articleSubTitle,articleContent) VALUES (3,'General Info','SOLID Principles in General','SOLID is a set of five design principles intended to guide software developers in creating more understandable, flexible, and maintainable code. The acronym stands for Single Responsibility Principle (SRP), which dictates that a class should have only one reason to change, Open/Closed Principle (OCP), which states that software entities should be open for extension but closed for modification, Liskov Substitution Principle (LSP), which ensures that objects of a superclass should be replaceable with objects of a subclass without altering the correctness of the program, Interface Segregation Principle (ISP), which advocates for creating specific, client-focused interfaces rather than one general-purpose interface, and Dependency Inversion Principle (DIP), which advises that high-level modules should not depend on low-level modules, but both should depend on abstractions. Together, these principles aim to improve the design and architecture of software, making it easier to manage and evolve over time.');
INSERT INTO testarticles(categoryId,articleTitle,articleSubTitle,articleContent) VALUES (3,'SRP','The Single Responsibility Principle (SRP)','The Single Responsibility Principle (SRP) is one of the fundamental principles in software design, stating that a class or module should have only one reason to change, meaning it should only have one job or responsibility. This principle helps in achieving a separation of concerns within a system, making the codebase easier to understand, maintain, and modify. By ensuring that each class has a single responsibility, developers can avoid unintended side effects when making changes, as each class will only encapsulate a specific part of the functionality. This leads to more modular, flexible, and testable code.');
INSERT INTO testarticles(categoryId,articleTitle,articleSubTitle,articleContent) VALUES (3,'OCP','The Open/Closed Principle (OCP)','The **Open/Closed Principle (OCP)** is a key concept in object-oriented design that states software entities such as classes, modules, and functions should be open for extension but closed for modification. This means that the behavior of a module can be extended without altering its existing code. By adhering to OCP, developers can add new functionality to a system without risking changes to the existing, tested code, thereby enhancing code stability and minimizing the chances of introducing bugs. This principle encourages the use of abstractions and interfaces, allowing systems to be more adaptable and scalable while preserving the integrity of the original code.');
INSERT INTO testarticles(categoryId,articleTitle,articleSubTitle,articleContent) VALUES (3,'LSP','The Liskov Substitution Principle (LSP)','The Liskov Substitution principle was introduced by Barbara Liskov in her conference keynote “Data abstraction” in 1987. A few years later, she published a paper with Jeanette Wing in which they defined the principle as: Let Φ(x) be a property provable about objects x of type T. Then Φ(y) should be true for objects y of type S where S is a subtype of T. 
 The principle defines that objects of a superclass shall be replaceable with objects of its subclasses without breaking the application. That requires the objects of your subclasses to behave in the same way as the objects of your superclass. You can achieve that by following a few rules, which are pretty similar to the design by contract concept defined by Bertrand Meyer. 
 The Liskov Substitution Principle (LSP) actually states that you must be able to substitute a sub-object for its base object without breaking the program. It implies that there should be a “contract” between someone who creates an abstraction and those who use it.');
INSERT INTO testarticles(categoryId,articleTitle,articleSubTitle,articleContent) VALUES (3,'ISP','The Interface Segregation Principle (ISP)','he Interface Segregation Principle (ISP) is a design principle that emphasizes the significance of creating interfaces tailored to specific functionalities rather than complex interfaces that include a wide range of unrelated functions. The main goal of ISP is to ensure that clients get only the needed details. This makes code more efficient and easier to maintain. 
 Practicaly it means that, big interfaces should be split into smaller relevant interfaces. If there is one big interface when new functionality is added to that interface, all classes that implement this interface would be forced to implement new functionality regardless of whether they support it or not. If there is a dependency between two classes, one class should depend on another class via the smallest possible interface.');
INSERT INTO testarticles(categoryId,articleTitle,articleSubTitle,articleContent) VALUES (3,'DI - IoC','Dependency Injection (DI) - Inversion of Control (IoC)','Dependency injection is a fundamental concept in software design that helps manage dependencies between different components of a system. By decoupling the creation of a component from its dependencies, dependency injection promotes better code maintainability, testability, and scalability.');
INSERT INTO testarticles(categoryId,articleTitle,articleSubTitle,articleContent) VALUES (4,'Master','Scrum Master','A Scrum Master is a key role in Agile development, particularly within the Scrum framework, which is a popular methodology for managing complex projects. The Scrum Master is responsible for ensuring that the Scrum team adheres to Agile practices and principles, facilitating smooth operation and continuous improvement. They act as a servant-leader, helping the team work efficiently by removing obstacles, ensuring that communication flows freely, and fostering a collaborative environment. Unlike traditional project managers, Scrum Masters do not have authority over the team but instead support and empower them to self-organize and make decisions. 
 The Scrum Master also serves as a liaison between the team and external stakeholders, protecting the team from unnecessary distractions and interruptions. They facilitate key Scrum ceremonies such as daily stand-ups, sprint planning, reviews, and retrospectives, ensuring that these meetings are productive and focused. By guiding the team through the Scrum process and fostering a culture of continuous improvement, the Scrum Master helps maximize the team''s productivity and the quality of the product being developed. Their role is critical in helping the team adapt to changes, maintain focus on the product goals, and deliver value incrementally.');
INSERT INTO testarticles(categoryId,articleTitle,articleSubTitle,articleContent) VALUES (4,'PO','Product Owner (PO)','In Scrum Agile development, the **Product Owner** is a crucial role responsible for defining the features and functionalities of the product, representing the stakeholders, and maximizing the value of the product delivered by the Scrum team. The Product Owner owns the product backlog, which is a prioritized list of user stories, features, and tasks that the team needs to work on. They are responsible for ensuring that the backlog is visible, transparent, and clearly communicated to all team members. The Product Owner prioritizes the backlog based on business value, customer needs, and market conditions, ensuring that the most valuable work is completed first. 
 A key part of the Product Owner''s role is to maintain a clear vision of what the product should do and why it is valuable, thus guiding the team towards the desired outcome. They work closely with stakeholders, including customers, users, and the business, to gather requirements and feedback. The Product Owner also collaborates with the development team during sprint planning, ensuring that the team understands the product goals and the requirements of the backlog items. By making critical decisions about scope and priorities, the Product Owner helps ensure that the team delivers valuable increments of the product that align with the overall strategic objectives.');
INSERT INTO testarticles(categoryId,articleTitle,articleSubTitle,articleContent) VALUES (4,'Backlog','Backlog','In Scrum Agile development, the Backlog is a dynamic, prioritized list of work items that the Scrum team needs to address. It serves as the single source of truth for all the tasks, features, enhancements, and bug fixes that need to be completed to develop and maintain the product. The Backlog is divided into two main types: the Product Backlog and the Sprint Backlog. 
 Product Backlog: Managed by the Product Owner, the Product Backlog contains all the desired work for the product, sorted by priority. It is an ever-evolving list that reflects the product''s future work and changes as new information is obtained. The items in the Product Backlog, often referred to as ''backlog items'' or ''user stories'', are continuously refined and reprioritized to ensure the most valuable and relevant work is being tackled first. 
 Sprint Backlog: At the start of each sprint, the development team selects a set of items from the top of the Product Backlog to focus on during that sprint. These selected items form the Sprint Backlog, which is a more granular, actionable list of tasks that the team commits to completing within the sprint. The Sprint Backlog includes not only the user stories but also the detailed tasks and technical work required to deliver those stories, giving the team a clear plan for the sprint. 
 The Backlog plays a critical role in Scrum, providing transparency and a shared understanding of the work needed to build the product. It enables the Scrum team to be flexible and responsive to changes, ensuring that the most valuable work is always being prioritized and delivered.');
INSERT INTO testarticles(categoryId,articleTitle,articleSubTitle,articleContent) VALUES (4,'User Stories','User Stories - Story Points','User Stories and Story Points are fundamental components in Scrum development, playing crucial roles in defining and estimating the work to be done by the Scrum team. 
  User Stories are short, simple descriptions of a feature or functionality written from the perspective of the end-user or customer. They are intended to capture the ''who,'' ''what,'' and ''why'' of a feature, focusing on the user''s needs rather than technical details. A typical user story follows the format: ''As a [type of user], I want [some goal] so that [some reason].'' This structure helps the development team understand the value and purpose of the feature, ensuring that the delivered product aligns with user needs and business goals. User stories are central to the Product Backlog and serve as the primary input for planning and prioritization in Scrum. 
  Story Points are a unit of measure used to express the overall effort required to implement a user story. Unlike time-based estimates, story points represent the relative complexity, risk, and amount of work involved in a story. During estimation sessions, the Scrum team collaboratively assigns story points to each user story, typically using a scale like the Fibonacci sequence (1, 2, 3, 5, 8, etc.) to indicate effort. This method allows the team to account for uncertainties and variations in task difficulty without getting bogged down in precise time estimates. Story points help the team gauge their velocity—the rate at which they complete work—and plan their capacity for future sprints, making them a key tool for effective sprint planning and workload management in Scrum.');
INSERT INTO testarticles(categoryId,articleTitle,articleSubTitle,articleContent) VALUES (4,'Sprint','Sprint','n Scrum development, a Sprint is a time-boxed iteration during which a specific set of tasks from the product backlog are completed. Sprints typically last between one and four weeks, with a consistent duration that allows the team to establish a regular cadence of work. At the beginning of each sprint, a Sprint Planning meeting is held where the Scrum team selects and commits to a set of user stories or tasks, known as the Sprint Backlog, based on their estimated capacity and priority. The goal of a sprint is to deliver a potentially shippable product increment—an improved version of the product that includes new features, bug fixes, or enhancements. 
 During the sprint, the team works collaboratively to complete the planned work, participating in daily Scrum meetings to track progress, identify obstacles, and adjust as needed. At the end of the sprint, a Sprint Review is conducted to showcase the completed work to stakeholders and gather feedback. This is followed by a Sprint Retrospective, where the team reflects on the sprint process, discusses what went well, what didn''t, and identifies opportunities for improvement. The iterative nature of sprints in Scrum promotes continuous delivery and incremental progress, allowing the team to adapt to changing requirements and deliver high-quality products efficiently.');
INSERT INTO testarticles(categoryId,articleTitle,articleSubTitle,articleContent) VALUES (4,'Meetings','SCRUM Meetings','In Scrum development, several meetings (also known as ceremonies) are essential to ensure effective communication, collaboration, and progress tracking. Here’s an overview: 
 Required Meetings (Ceremonies) 
 1. Sprint Planning: 
 Purpose: To define the work to be done during the upcoming sprint. Attendees: Product Owner, Scrum Master, and Development Team. 
 Key Activities: Selecting items from the Product Backlog for the Sprint Backlog, defining the Sprint Goal, and planning how to accomplish the work. 
 
 2. Daily Scrum: 
 Purpose: To synchronize the team''s work and plan for the next 24 hours. /n Attendees: Development Team and Scrum Master (Product Owner is optional). /n Key Activities: Each team member answers three questions: What did I do yesterday? What will I do today? Are there any impediments in my way? 
 
 3. Sprint Review: 
 Purpose: To inspect the increment and adapt the Product Backlog based on feedback. 
 Attendees: Scrum Team and Stakeholders. 
 Key Activities: Demonstrating the work completed during the sprint, discussing the product increment, and receiving feedback. 
 
 4. Sprint Retrospective: 
 Purpose: To reflect on the past sprint and identify areas for improvement. 
 Attendees: Scrum Team. 
 Key Activities: Reviewing what went well, what didn’t, and how to improve processes and interactions in the next sprint.');
INSERT INTO testarticles(categoryId,articleTitle,articleSubTitle,articleContent) VALUES (4,'Artifacts','SCRUM Artifacts','1. Product Backlog: 
 Description: An ordered list of all desired work on the project, maintained by the Product Owner. It includes features, enhancements, fixes, and other requirements. 
 Purpose: To provide a single source of requirements for changes to be made to the product. 
 
 2. Sprint Backlog: 
 Description: A list of tasks and requirements that the team commits to completing in a sprint, derived from the Product Backlog. 
 Purpose: To make visible the work that the Development Team will perform during the sprint. 
 
 3. Increment: 
 Description: The sum of all the Product Backlog items completed during a sprint and the value of the increments of all previous sprints. 
 Purpose: To provide a measure of progress and value delivered, meeting the definition of ''Done''. 
 
 4. Definition of Done (DoD): 
 Description: A shared understanding among the Scrum Team of what it means for work to be complete. 
 Purpose: To ensure the increment is usable and meets the quality standards required for release. 
 
 These artifacts are foundational to Scrum''s framework, promoting transparency, inspection, and adaptation throughout the development process.');
INSERT INTO testarticles(categoryId,articleTitle,articleSubTitle,articleContent) VALUES (5,'Introduction','Introduction To Microservices','Microservices are an architectural style in software development that structures an application as a collection of small, autonomous services, each responsible for a specific business function. Unlike monolithic architectures, where all components are tightly interwoven into a single codebase, microservices are designed to be loosely coupled and independently deployable. Each microservice operates as a separate process and communicates with others over lightweight protocols, typically HTTP or messaging queues. This separation allows teams to develop, deploy, and scale each service independently, enabling a more flexible and agile development process. Technologies like Docker and Kubernetes often support microservices, providing containerization and orchestration capabilities to manage the complexities of deploying multiple services. 
 
 The microservices architecture promotes several key benefits, such as improved scalability, resilience, and agility. Since services can be scaled independently, resources can be allocated precisely where demand is highest, optimizing performance and cost-efficiency. The isolated nature of microservices also enhances system resilience, as the failure of one service is less likely to cascade and impact others. Furthermore, the architecture supports a decentralized governance model, allowing teams to choose the best technologies and development practices for their specific service. However, this approach also introduces challenges, including increased complexity in managing inter-service communication, data consistency, and overall system monitoring. Effective implementation requires robust DevOps practices and a well-planned strategy for service orchestration, monitoring, and fault tolerance.');
INSERT INTO testarticles(categoryId,articleTitle,articleSubTitle,articleContent) VALUES (5,'Circuit Braker','Circuit Braker','In the context of microservices, a Circuit Breaker is a design pattern used to detect failures and encapsulate the logic of preventing a failure from constantly recurring, thereby making a system more resilient. The pattern is inspired by electrical circuit breakers used to prevent overloads; in software, it prevents an application from performing an operation that is likely to fail. When a service detects that a particular downstream service or dependency is experiencing failures or high latency, the circuit breaker will ''trip'' and automatically stop further requests to the failing service. During this period, any attempt to access the failing service will immediately result in an error, rather than trying to make a potentially doomed network call. This not only helps in preventing system overload but also allows the failing service time to recover. 
 
 The Circuit Breaker pattern typically includes three states: Closed, Open, and Half-Open. In the Closed state, the circuit breaker allows requests to flow normally until a certain threshold of failures is reached. When the failure threshold is exceeded, the circuit transitions to the Open state, where requests are blocked for a specified timeout period, preventing further attempts that are likely to fail. After the timeout expires, the circuit breaker enters the Half-Open state, allowing a limited number of test requests to pass through. If these requests are successful, the circuit breaker transitions back to the Closed state, restoring normal operation. However, if failures continue, it reverts to the Open state. This mechanism not only prevents cascading failures and system degradation but also helps to maintain the stability and reliability of the overall microservices ecosystem.');
INSERT INTO testarticles(categoryId,articleTitle,articleSubTitle,articleContent) VALUES (5,'CQRS','CQRS (Command Query Responsibility Segregation)','CQRS (Command Query Responsibility Segregation) is a design pattern often used in microservices architecture to separate the operations that read data (queries) from the operations that update data (commands). In traditional systems, the same data model is typically used for both reading and writing, which can lead to complex code and performance issues. CQRS addresses these concerns by using distinct models for handling read and write operations. The command model, which handles updates, focuses on the business logic and validation rules required to process changes, while the query model is optimized for fetching data, often using different representations tailored for specific query requirements. This separation allows for more scalable, maintainable, and flexible systems, as each model can be optimized independently. 
 
 In a microservices context, CQRS can significantly enhance system scalability and performance. By decoupling the read and write sides, different microservices can handle specific responsibilities, reducing contention and improving responsiveness. For instance, write-heavy services can be optimized for transactional integrity and consistency, while read-heavy services can be scaled out to serve large volumes of queries efficiently. Additionally, CQRS often pairs well with Event Sourcing, where state changes are stored as a sequence of events. This combination allows for a clear audit trail and the ability to reconstruct the state of the system at any point in time. However, implementing CQRS can introduce complexity, particularly in managing consistency between the read and write models, and requires careful consideration of eventual consistency and data synchronization strategies.');
INSERT INTO testarticles(categoryId,articleTitle,articleSubTitle,articleContent) VALUES (5,'Event Sourcing','Event Sourcing','The primary idea behind utilizing CQRS with the Event Sourcing design is to store events in the write database, which will serve as the events database that is the source of truth. Following that, materialized views of the data with denormalized tables are provided by the read database of the CQRS design pattern. Naturally, events from the write database are consumed by the materialized views read database and transformed into denormalized views. 
 
 Allow me to define the Event Sourcing Pattern at the outset. As you are aware, the majority of apps store data with the entity''s current state in databases. For instance, the user email box would update with the new address if the user changed it while utilizing our service. Thus, email information takes precedence over the user table''s current email field. We are always aware of the most recent state of the data in this way. 
 
 Applying the Event Sourcing pattern means altering the way that data is saved into databases. The Event Sourcing pattern enables to save all events with a sequential ordering of data events into a database, as opposed to recording the most recent status of data. We name this database of events, the event store. It adds each modification to a sequential list of events rather than changing the status of a data record. 
 
 The Event Store thus becomes the data''s ultimate source of accuracy. The event store is then converted to a read database using the materialized views technique. Message broker systems can handle this conversion process by using a publish/subscribe pattern with a publish event. Additionally, this event list allows you to play back occurrences at a certain timestamp, which allows it to generate the most recent data status.');
INSERT INTO testarticles(categoryId,articleTitle,articleSubTitle,articleContent) VALUES (5,'API Gateway','API Gateway','Applications use APIs as the front-door to obtain data and business logic from backend services. An API is basically the interface that a piece of software offers to other people or programs so they can interact with it, as defined here. 
 Selecting a programming language (Java, Python, PHP, etc.) to write the API logic in is necessary while building an API. In addition, you must install the API on a server and keep an eye on it to make sure your infrastructure can handle a high volume of queries. 
 These stages are abstracted away via API gateways. It eliminates the need for you to manage the underlying infrastructure or write a lot of code. All you have to do is set up API endpoints where clients can submit requests. 
 An API gateway is a fully managed service that makes it easier for developers to create, publish, maintain, monitor, and secure APIs at almost any scale. 
 All major cloud providers have fully managed API gateway services: /n AWS API Gateway 
 GCP API Gateway 
 Azure API Management 
 The term “fully managed” in the context of cloud computing means that the maintenance and management responsibilities of the service are handled by the cloud provider. This means the underlying infrastructure, software updates, security, scalability, availability and disaster recovery are all managed by the cloud provider.');
COMMIT;


-- ------------------------------------
-- 250612
-- ------------------------------------
BEGIN;
INSERT INTO `content_types` (`cont_type_identifier`) VALUES ('plain');
INSERT INTO `content_types` (`cont_type_identifier`) VALUES ('mark');
INSERT INTO `content_types` (`cont_type_identifier`) VALUES ('html');
COMMIT;
